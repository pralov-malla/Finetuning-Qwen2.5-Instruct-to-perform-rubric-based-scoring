{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdfqSsPYHxmo"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q --upgrade requests==2.32.3 bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 datasets==3.2.0 peft==0.14.0 trl==0.14.0 matplotlib wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoE9ANiQ0EOi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch, json\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import wandb\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO0GsOt00Jrb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT5ONS0N1NVB"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/datasets/splits/train.csv')\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/datasets/splits/val.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/datasets/splits/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEfAx5Ss2K4b"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"No GPU! Go to Runtime > Change runtime type > GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXjS46Wr4XXu"
      },
      "source": [
        "Keeping only necessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3chtfuO2ZAY"
      },
      "outputs": [],
      "source": [
        "KEEP_COLS = [\"image_id\",\"image\",\"content\",\"scores_json\"]\n",
        "\n",
        "train_df = train_df[KEEP_COLS]\n",
        "val_df = val_df[KEEP_COLS]\n",
        "test_df = test_df[KEEP_COLS]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq-CwWYmfulP"
      },
      "source": [
        "building a function to reduce the JSON according the visual_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7dGYnwBfnvA"
      },
      "outputs": [],
      "source": [
        "REDUCE_JSON = True  # toggle to compact the task JSON to save tokens\n",
        "\n",
        "# Optional: lightweight reducer per visual_type (safe defaults)\n",
        "def drop_empty(x):\n",
        "    if isinstance(x, dict):\n",
        "        out = {k: drop_empty(v) for k, v in x.items() if v not in (None, [], {})}\n",
        "        return {k: v for k, v in out.items() if v not in ([], {})}\n",
        "    if isinstance(x, list):\n",
        "        out = [drop_empty(v) for v in x if v not in (None, [], {})]\n",
        "        return [v for v in out if v not in ([], {})]\n",
        "    return x\n",
        "\n",
        "\n",
        "def reduce_task_json(meta: dict) -> dict:\n",
        "    # minimal keep per type\n",
        "    def reduce_table(struct):\n",
        "        r = struct.get(\"row_headers\") if isinstance(struct.get(\"row_headers\"), list) else None\n",
        "        c = struct.get(\"column_headers\") if isinstance(struct.get(\"column_headers\"), list) else None\n",
        "        vals = struct.get(\"values\") if isinstance(struct.get(\"values\"), list) else None\n",
        "        out = {\"row_headers\": r, \"column_headers\": c}\n",
        "        if r and c and vals:\n",
        "            cell_map = {}\n",
        "            for cell in vals:\n",
        "                if not isinstance(cell, dict):\n",
        "                    continue\n",
        "                rr, cc = cell.get(\"row\"), cell.get(\"column\")\n",
        "                if rr is None or cc is None:\n",
        "                    continue\n",
        "                cell_map.setdefault(rr, {})[cc] = cell.get(\"value\")\n",
        "            out[\"matrix\"] = [[cell_map.get(rr, {}).get(cc) for cc in c] for rr in r]\n",
        "        return drop_empty(out)\n",
        "\n",
        "    def reduce_bar(struct):\n",
        "        cats = struct.get(\"categories\") if isinstance(struct.get(\"categories\"), list) else None\n",
        "        series = struct.get(\"series\") if isinstance(struct.get(\"series\"), list) else None\n",
        "        out = {\"bar_chart_type\": struct.get(\"bar_chart_type\"), \"orientation\": struct.get(\"orientation\"), \"categories\": cats}\n",
        "        if cats and series:\n",
        "            out[\"series\"] = []\n",
        "            for s in series:\n",
        "                if not isinstance(s, dict):\n",
        "                    continue\n",
        "                data = s.get(\"data\") if isinstance(s.get(\"data\"), list) else []\n",
        "                mp = {}\n",
        "                for d in data:\n",
        "                    if not isinstance(d, dict):\n",
        "                        continue\n",
        "                    cat = d.get(\"category\")\n",
        "                    if cat is None:\n",
        "                        continue\n",
        "                    mp[cat] = d.get(\"value\")\n",
        "                out[\"series\"].append({\"label\": s.get(\"label\"), \"values\": [mp.get(c) for c in cats]})\n",
        "        return drop_empty(out)\n",
        "\n",
        "    def reduce_line(struct):\n",
        "        xl = struct.get(\"x_labels\") if isinstance(struct.get(\"x_labels\"), list) else None\n",
        "        series = struct.get(\"series\") if isinstance(struct.get(\"series\"), list) else None\n",
        "        out = {\"x_axis_type\": struct.get(\"x_axis_type\"), \"x_labels\": xl, \"y_unit\": struct.get(\"y_unit\")}\n",
        "        if xl and series:\n",
        "            out[\"series\"] = []\n",
        "            for s in series:\n",
        "                if not isinstance(s, dict):\n",
        "                    continue\n",
        "                pts = s.get(\"points\") if isinstance(s.get(\"points\"), list) else []\n",
        "                yvals = [None] * len(xl)\n",
        "                for i, p in enumerate(pts):\n",
        "                    if i >= len(xl) or not isinstance(p, dict):\n",
        "                        break\n",
        "                    yvals[i] = p.get(\"y_value\")\n",
        "                out[\"series\"].append({\"label\": s.get(\"label\"), \"y_values\": yvals})\n",
        "        return drop_empty(out)\n",
        "\n",
        "    def reduce_pie(struct):\n",
        "        slices = struct.get(\"slices\") if isinstance(struct.get(\"slices\"), list) else None\n",
        "        out = {\"context_label\": struct.get(\"context_label\"), \"is_donut_chart\": struct.get(\"is_donut_chart\")}\n",
        "        if slices:\n",
        "            out[\"slices\"] = [{\"label\": s.get(\"label\"), \"percentage\": s.get(\"percentage\")} for s in slices if isinstance(s, dict)]\n",
        "        return drop_empty(out)\n",
        "\n",
        "    def reduce_process(struct):\n",
        "        stages = struct.get(\"stages\") if isinstance(struct.get(\"stages\"), list) else None\n",
        "        out = {\"process_title\": struct.get(\"process_title\"), \"is_cycle\": struct.get(\"is_cycle\")}\n",
        "        if stages:\n",
        "            out[\"stages\"] = [{\"name\": s.get(\"name\"), \"order_index\": s.get(\"order_index\")} for s in stages if isinstance(s, dict)]\n",
        "        return drop_empty(out)\n",
        "\n",
        "    def reduce_map(struct):\n",
        "        out = {\"base_region_description\": struct.get(\"base_region_description\")}\n",
        "        sc = struct.get(\"scenarios\")\n",
        "        if isinstance(sc, list):\n",
        "            out[\"scenarios\"] = []\n",
        "            for s in sc:\n",
        "                if not isinstance(s, dict):\n",
        "                    continue\n",
        "                feats = s.get(\"features\") if isinstance(s.get(\"features\"), list) else []\n",
        "                f_out = []\n",
        "                for f in feats:\n",
        "                    if not isinstance(f, dict):\n",
        "                        continue\n",
        "                    f_out.append({\"label\": f.get(\"label\"), \"type\": f.get(\"type\"), \"category\": f.get(\"category\"), \"status\": f.get(\"status\")})\n",
        "                out[\"scenarios\"].append({\"label\": s.get(\"label\"), \"features\": f_out})\n",
        "        out[\"changes_between_scenarios\"] = struct.get(\"changes_between_scenarios\")\n",
        "        out[\"summary\"] = struct.get(\"summary\")\n",
        "        return drop_empty(out)\n",
        "\n",
        "    if not isinstance(meta, dict):\n",
        "        return meta\n",
        "    tcat = meta.get(\"task_visual_category\")\n",
        "    visuals = meta.get(\"visuals\") if isinstance(meta.get(\"visuals\"), list) else []\n",
        "    if len(visuals) >= 2 and tcat != \"multiple_graphs\":\n",
        "        tcat = \"multiple_graphs\"\n",
        "\n",
        "    out = {\n",
        "        \"schema_version\": meta.get(\"schema_version\"),\n",
        "        \"task_visual_category\": tcat,\n",
        "        \"topic_context\": meta.get(\"topic_context\"),\n",
        "    }\n",
        "    if isinstance(meta.get(\"global_semantics\"), dict):\n",
        "        gs = meta[\"global_semantics\"]\n",
        "        out[\"global_semantics\"] = {\n",
        "            \"overview\": gs.get(\"overview\"),\n",
        "            \"key_features\": gs.get(\"key_features\"),\n",
        "            \"extremes\": gs.get(\"extremes\"),\n",
        "            \"comparisons\": gs.get(\"comparisons\"),\n",
        "        }\n",
        "\n",
        "    v_out = []\n",
        "    for v in visuals:\n",
        "        if not isinstance(v, dict):\n",
        "            continue\n",
        "        vtype = v.get(\"visual_type\")\n",
        "        struct = v.get(\"structure\") if isinstance(v.get(\"structure\"), dict) else {}\n",
        "        if vtype == \"table\":\n",
        "            s_red = reduce_table(struct)\n",
        "        elif vtype == \"bar_chart\":\n",
        "            s_red = reduce_bar(struct)\n",
        "        elif vtype == \"line_graph\":\n",
        "            s_red = reduce_line(struct)\n",
        "        elif vtype == \"pie_chart\":\n",
        "            s_red = reduce_pie(struct)\n",
        "        elif vtype == \"process_diagram\":\n",
        "            s_red = reduce_process(struct)\n",
        "        elif vtype == \"map\":\n",
        "            s_red = reduce_map(struct)\n",
        "        else:\n",
        "            s_red = struct\n",
        "        v_out.append(drop_empty({\n",
        "            \"visual_id\": v.get(\"visual_id\"),\n",
        "            \"visual_type\": vtype,\n",
        "            \"role\": v.get(\"role\"),\n",
        "            \"panel_label\": v.get(\"panel_label\"),\n",
        "            \"title\": v.get(\"title\"),\n",
        "            \"structure\": s_red,\n",
        "        }))\n",
        "    if v_out:\n",
        "        out[\"visuals\"] = v_out\n",
        "\n",
        "    if isinstance(meta.get(\"relationships_between_visuals\"), list):\n",
        "        out[\"relationships_between_visuals\"] = meta.get(\"relationships_between_visuals\")\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rr0JVST68ZJ"
      },
      "source": [
        "Building train/val/test JSONL in chat format for SFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbNR8cKZ63Od"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPZABsApesvf"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an IELTS Academic Writing Task 1 examiner.\n",
        "\n",
        "You will receive TWO inputs:\n",
        "1) TASK_PROMPT_JSON: a structured JSON description of the Task 1 visual(s). Treat this as the ONLY ground truth.\n",
        "2) CANDIDATE_ESSAY: the candidate’s full written response.\n",
        "\n",
        "Your job: produce rubric-based band scores for IELTS Writing Task 1.\n",
        "\n",
        "SCORING SCALE (STRICT)\n",
        "- Score each criterion in 0.5 steps.\n",
        "- Criterion score range: 0.0 to 9.0 (inclusive).\n",
        "- All criterion scores must be multiples of 0.5.\n",
        "- IMPORTANT: overall_band_score must NEVER be 9.0. Cap overall_band_score at 8.5.\n",
        "\n",
        "CRITERIA (score all four)\n",
        "1) task_response_score (TR)\n",
        "   - Describe what is shown; no opinions/causes/solutions unless shown.\n",
        "   - MUST include a clear overview of main trends/major features (missing/unclear overview lowers TR).\n",
        "   - Select key features and comparisons; avoid listing everything.\n",
        "   - Accuracy is critical: penalize invented data, wrong figures/units/time periods, or trends that contradict TASK_PROMPT_JSON.\n",
        "2) coherence_cohesion_score (CC)\n",
        "   - Logical paragraphing (intro + overview + grouped details), clear progression, appropriate linking.\n",
        "3) lexical_resource_score (LR)\n",
        "   - Precise academic reporting vocabulary; accurate collocations for data (rise to/by, remain stable, peak at, etc.); avoid repetition.\n",
        "4) grammatical_range_accuracy_score (GRA)\n",
        "   - Range + accuracy; frequent errors and awkward structures reduce score.\n",
        "\n",
        "WORD COUNT RULE\n",
        "- If the essay is clearly under ~150 words, apply a noticeable penalty (especially TR, and often CC).\n",
        "\n",
        "LOW-SCORE VERIFICATION (IMPORTANT)\n",
        "If your initial scoring suggests ANY criterion < 4.5, you MUST do a second, rigorous check BEFORE finalizing:\n",
        "A) Re-check TR basics: is there at least an attempt at paraphrase + overview + some data/features (even if weak)?\n",
        "B) Re-check whether errors are truly severe enough to justify <4.5 versus a weak-but-present response (≈4.5–5.0).\n",
        "C) Re-check that you are not over-penalizing for grammar/vocabulary when the task meaning is still recoverable.\n",
        "D) Only keep a score <4.5 if the response is clearly extremely limited (e.g., no real overview, very little/incorrect description, heavy invention, or meaning mostly unclear).\n",
        "\n",
        "OVERALL BAND (STRICT)\n",
        "- overall_band_score = average of the four criterion scores.\n",
        "- Round to the nearest 0.5.\n",
        "- If exactly halfway between two 0.5 steps (x.25 or x.75), round UP.\n",
        "- After rounding, if overall_band_score == 9.0, set overall_band_score = 8.5.\n",
        "\n",
        "OUTPUT FORMAT (STRICT)\n",
        "Return ONLY one valid JSON object with exactly these keys and numeric values (no extra keys, no explanation, no markdown, no surrounding text).\n",
        "\n",
        "{\n",
        "  \"overall_band_score\": number,\n",
        "  \"task_response_score\": number,\n",
        "  \"coherence_cohesion_score\": number,\n",
        "  \"lexical_resource_score\": number,\n",
        "  \"grammatical_range_accuracy_score\": number\n",
        "}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZfV3lH4b7gx"
      },
      "outputs": [],
      "source": [
        "def to_record(row):\n",
        "    task_obj_dict = json.loads(row[\"image\"])\n",
        "    if REDUCE_JSON:\n",
        "        task_json_for_user_payload = reduce_task_json(task_obj_dict)\n",
        "    else:\n",
        "        task_json_for_user_payload = task_obj_dict\n",
        "\n",
        "    user_payload = {\"task_json\": task_json_for_user_payload, \"student_essay\": row[\"content\"]}\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": json.dumps(user_payload, ensure_ascii=False)},\n",
        "            {\"role\": \"assistant\", \"content\": row[\"scores_json\"]},\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hteOxuhZf74N"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
        "\n",
        "# datasets/ is directly inside MyDrive:\n",
        "PROJECT_ROOT = DRIVE_ROOT\n",
        "\n",
        "SPLITS_DIR = PROJECT_ROOT / \"datasets\" / \"splits\"\n",
        "OUT_DIR = SPLITS_DIR / \"jsonl_chat\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUT_DIR_RUN = PROJECT_ROOT / \"runs\" / \"qwen25_ielts_task1_qlora_balanced\"\n",
        "OUT_DIR_RUN.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEGa22PIbtdg"
      },
      "outputs": [],
      "source": [
        "print(\"CWD:\", Path.cwd())\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n",
        "print(\"OUT_DIR_RUN:\", OUT_DIR_RUN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd4-gH-cgY6j"
      },
      "outputs": [],
      "source": [
        "def write_jsonl(df, path: Path):\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for _, r in df.iterrows():\n",
        "            f.write(json.dumps(to_record(r), ensure_ascii=False) + \"\\n\")\n",
        "    print(f\"Wrote {len(df)} -> {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWTCnhWZgWbX"
      },
      "outputs": [],
      "source": [
        "TRAIN_JSONL = OUT_DIR / \"train.jsonl\"\n",
        "VAL_JSONL   = OUT_DIR / \"val.jsonl\"\n",
        "TEST_JSONL  = OUT_DIR / \"test.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv7fkIGDgc56"
      },
      "outputs": [],
      "source": [
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "write_jsonl(train_df, TRAIN_JSONL)\n",
        "write_jsonl(val_df, VAL_JSONL)\n",
        "write_jsonl(test_df, TEST_JSONL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqVUF80D2pwZ"
      },
      "source": [
        "Checking token size of full `image` JSON, and `reduced image` JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmcA5xBsb8EM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "sample = train_df.sample(n=min(500, len(train_df)), random_state=42)\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
        "\n",
        "def payload(row, reduce=False):\n",
        "    # Parse image JSON string to a Python dict\n",
        "    task_obj_dict = json.loads(row[\"image\"])\n",
        "\n",
        "    # Handle reduction if requested\n",
        "    if reduce:\n",
        "        task_json_for_user_payload = reduce_task_json(task_obj_dict)\n",
        "    else:\n",
        "        task_json_for_user_payload = task_obj_dict\n",
        "\n",
        "    # student_essay is already a string\n",
        "    student_essay = row[\"content\"]\n",
        "\n",
        "    # Parse scores_json string to a Python dict\n",
        "    scores_dict = json.loads(row[\"scores_json\"])\n",
        "\n",
        "    # The user content for the chat template\n",
        "    user_content_dict = {\"task_json\": task_json_for_user_payload, \"student_essay\": student_essay}\n",
        "    user_content_str = json.dumps(user_content_dict, ensure_ascii=False)\n",
        "\n",
        "    # The messages list for apply_chat_template\n",
        "    msgs = [\n",
        "      {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
        "      {\"role\":\"user\",\"content\": user_content_str},\n",
        "      {\"role\":\"assistant\",\"content\": json.dumps(scores_dict, ensure_ascii=False)},\n",
        "    ]\n",
        "    # `apply_chat_template` with `tokenize=True` returns a list of token IDs directly\n",
        "    return tokenizer.apply_chat_template(msgs, tokenize=True, add_generation_prompt=False)\n",
        "\n",
        "lens_full = [len(payload(r, reduce=False)) for _, r in sample.iterrows()]\n",
        "print(\"p95 full:\", int(np.percentile(lens_full, 95)))\n",
        "lens_red = [len(payload(r, reduce=True)) for _, r in sample.iterrows()]\n",
        "print(\"p95 reduced:\", int(np.percentile(lens_red, 95)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKB5N3vA3I8V"
      },
      "source": [
        "Building dataset and balancing weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lxMfIJA3MU-"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LEN = 3500\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IdVTw8k4Nrh"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\"train\": str(TRAIN_JSONL), \"validation\": str(VAL_JSONL)},\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_KFMU71_c6C"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc5DcAyA_irs"
      },
      "outputs": [],
      "source": [
        "ds['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJPHsNGI_tiL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "print(json.dumps(ds[\"train\"][0], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsXBjB1L8q2p"
      },
      "outputs": [],
      "source": [
        "def formatting_func(example):\n",
        "    return tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDTofZI097R9"
      },
      "outputs": [],
      "source": [
        "formatted = formatting_func(ds[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG4BET5u_9V8"
      },
      "outputs": [],
      "source": [
        "print(formatted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUEapQ4nPMpL"
      },
      "source": [
        "# QLoRA model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLjr-PjPPKlx"
      },
      "outputs": [],
      "source": [
        "compute_dtype = (\n",
        "    torch.bfloat16\n",
        "    if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "    else torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RjiHE2ZPYhq"
      },
      "outputs": [],
      "source": [
        "compute_dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhbhURT6QOMy"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2IrPRj2QeOs"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdTHtZAZUS5Y"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOboxuBVSK14"
      },
      "outputs": [],
      "source": [
        "model.config.use_cache = False  # needed with gradient checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYcCoLniVzjp"
      },
      "source": [
        "Trainer Config(baseline, no weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfw5BZjtcqHX"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "HF_USER = \"pralovmalla\"\n",
        "PROJECT_NAME = \"qwen2.5-IELTS-writing-task1\"\n",
        "\n",
        "# Run name for saving the model in the hub\n",
        "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "LOG_TO_WANDB = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIPk1dvHbraZ"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9Ged6P2YuRS"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88kStlj0fnU-"
      },
      "outputs": [],
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jncMWUZGaPCj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"gradients\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M9hgxccgtOR"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW6IFVJm9tU9"
      },
      "outputs": [],
      "source": [
        "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLdfoYG59uVd"
      },
      "outputs": [],
      "source": [
        "bf16_ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEqNmLtZVyfA"
      },
      "outputs": [],
      "source": [
        "args = SFTConfig(\n",
        "    # --- admin / outputs ---\n",
        "    output_dir=str(OUT_DIR_RUN),          # folder to save checkpoints/adapters\n",
        "    run_name=RUN_NAME,\n",
        "\n",
        "    # --- training length ---\n",
        "    num_train_epochs=2,\n",
        "    max_steps=-1,\n",
        "\n",
        "    # --- memory-safe defaults for QLoRA ---\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "\n",
        "    # --- stability ---\n",
        "    learning_rate=1e-4,                  # changed from 2e-4\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.0,\n",
        "    max_grad_norm=0.3,\n",
        "\n",
        "    # --- efficiency ---\n",
        "    group_by_length=True,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",           # more stable than paged_adamw_8bit for many setups\n",
        "\n",
        "    # --- precision (L4: bf16, T4: fp16) ---\n",
        "    bf16=bf16_ok,\n",
        "    fp16=not bf16_ok,\n",
        "\n",
        "    # --- sequence length ---\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "\n",
        "    # --- logging / eval / saving (so W&B shows charts early) ---\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,\n",
        "    logging_first_step=True,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=20,\n",
        "\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=20,\n",
        "    save_total_limit=2,\n",
        "\n",
        "    # --- W&B ---\n",
        "    report_to=\"wandb\" if LOG_TO_WANDB else \"none\",\n",
        "\n",
        "    # --- Hub ---\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=HUB_MODEL_NAME,\n",
        "    hub_private_repo=True,\n",
        "    hub_strategy=\"end\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKliQgkILcGJ"
      },
      "source": [
        "Using `Data Collator`  to ensure that the model is trained to predict only the assistant's score JSON, not the whole prompt/content (system + user + visual JSON + essay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZkJmHz3K7YW"
      },
      "outputs": [],
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "\n",
        "response_template = \"<|im_start|>assistant\\n\"\n",
        "\n",
        "collator = DataCollatorForCompletionOnlyLM(\n",
        "    response_template=response_template,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHGdKiY7n3j9"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset=ds[\"validation\"],\n",
        "    args=args,\n",
        "    peft_config=peft_config,\n",
        "    formatting_func=formatting_func,   # chat formatting\n",
        "    data_collator=collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EaE5HzerAMuM"
      },
      "outputs": [],
      "source": [
        "# Fine-tune!\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model()  # Saves the fine-tuned LoRA/QLoRA adapter files into: <PROJECT_ROOT>/runs/qwen25_ielts_task1_qlora_balanced\n",
        "tokenizer.save_pretrained(args.output_dir)\n",
        "# Push our fine-tuned model to Hugging Face\n",
        "trainer.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
        "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKYwyVKwFcRq"
      },
      "outputs": [],
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}