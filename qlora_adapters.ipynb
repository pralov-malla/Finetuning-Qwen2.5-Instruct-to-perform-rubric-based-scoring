{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pralov-malla/Finetuning-Qwen2.5-Instruct-to-perform-rubric-based-scoring/blob/main/qlora_adapters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTyeC587jozL"
      },
      "outputs": [],
      "source": [
        "import os, shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "YNdCVdUTpWZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_ckpt = \"/content/drive/MyDrive/runs/qwen25_ielts_task1_qlora_balanced/checkpoint-480\"\n",
        "export_dir = \"/content/drive/MyDrive/runs/qwen25_ielts_task1_qlora_balanced/best_adapter_ckpt480\"\n",
        "os.makedirs(export_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "yV8OC3Iom-fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep = [\n",
        "    \"adapter_model.safetensors\",\n",
        "    \"adapter_config.json\",\n",
        "    \"tokenizer.json\",\n",
        "    \"tokenizer_config.json\",\n",
        "    \"special_tokens_map.json\",\n",
        "    \"added_tokens.json\",\n",
        "    \"vocab.json\",\n",
        "    \"merges.txt\",\n",
        "    \"training_args.bin\",\n",
        "]"
      ],
      "metadata": {
        "id": "nVVVObKvnBLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for f in keep:\n",
        "    src = os.path.join(best_ckpt, f)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, os.path.join(export_dir, f))"
      ],
      "metadata": {
        "id": "ebIawnkjoRxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Exported files:\", os.listdir(export_dir))"
      ],
      "metadata": {
        "id": "mOfFd6KroeJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U transformers accelerate peft bitsandbytes safetensors tqdm scikit-learn"
      ],
      "metadata": {
        "id": "J_gbmN7XrTI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "n9YwCyn9Mx5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, HfApi"
      ],
      "metadata": {
        "id": "QGDgahWIrXTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "tEa9PqcarcIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# constants\n",
        "HF_USER = \"pralovmalla\"\n",
        "PROJECT_NAME = \"qwen2.5-IELTS-writing-task1-best-checkpoint-480\"\n",
        "repo_id = f\"{HF_USER}/{PROJECT_NAME}\""
      ],
      "metadata": {
        "id": "fpIce9f3rgTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = f\"{HF_USER}/{PROJECT_NAME}\"\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id, private = True, exist_ok = True)"
      ],
      "metadata": {
        "id": "SV1OpNLsr359"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api.upload_folder(folder_path=export_dir, repo_id=repo_id, repo_type=\"model\")\n",
        "print(\"Uploaded:\", repo_id)"
      ],
      "metadata": {
        "id": "yf6UO7LosOZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using huggingface for inference"
      ],
      "metadata": {
        "id": "tNOHdieltYBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "adS7qwqktrdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "ADAPTER_REPO = f\"{HF_USER}/{PROJECT_NAME}\""
      ],
      "metadata": {
        "id": "6NWCxM0vt_mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")"
      ],
      "metadata": {
        "id": "qq66S6Z6uZGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.truncation_side = \"left\""
      ],
      "metadata": {
        "id": "yBydKZxOn3kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "id": "MwXDiAUIuv-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(base, ADAPTER_REPO, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "axlnk-Isu5Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "qZuLobLT0QDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, setting up for inference"
      ],
      "metadata": {
        "id": "Qj4aVsX62_Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "0fZvoyky3CHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "O5aj2X3e3Lu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting test.csv path\n",
        "CSV_PATH = \"/content/drive/MyDrive/datasets/splits/test.csv\"\n",
        "test_df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# setting predicted csv path\n",
        "OUT_CSV = \"/content/drive/MyDrive/datasets/splits/pred_test.csv\""
      ],
      "metadata": {
        "id": "YzCmuIiA2Op7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KEYS = [\n",
        "    \"overall_band_score\",\n",
        "    \"task_response_score\",\n",
        "    \"coherence_cohesion_score\",\n",
        "    \"lexical_resource_score\",\n",
        "    \"grammatical_range_accuracy_score\",\n",
        "]"
      ],
      "metadata": {
        "id": "1jjse1RyY4F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Optional: lightweight reducer per visual_type (safe defaults)\n",
        "def drop_empty(x):\n",
        "    if isinstance(x, dict):\n",
        "        out = {k: drop_empty(v) for k, v in x.items() if v not in (None, [], {})}\n",
        "        return {k: v for k, v in out.items() if v not in ([], {})}\n",
        "    if isinstance(x, list):\n",
        "        out = [drop_empty(v) for v in x if v not in (None, [], {})]\n",
        "        return [v for v in out if v not in ([], {})]\n",
        "    return x\n",
        "\n",
        "\n",
        "def reduce_task_json(meta: dict) -> dict:\n",
        "    # minimal keep per type\n",
        "    def reduce_table(struct):\n",
        "        r = struct.get(\"row_headers\") if isinstance(struct.get(\"row_headers\"), list) else None\n",
        "        c = struct.get(\"column_headers\") if isinstance(struct.get(\"column_headers\"), list) else None\n",
        "        vals = struct.get(\"values\") if isinstance(struct.get(\"values\"), list) else None\n",
        "        out = {\"row_headers\": r, \"column_headers\": c}\n",
        "        if r and c and vals:\n",
        "            cell_map = {}\n",
        "            for cell in vals:\n",
        "                if not isinstance(cell, dict):\n",
        "                    continue\n",
        "                rr, cc = cell.get(\"row\"), cell.get(\"column\")\n",
        "                if rr is None or cc is None:\n",
        "                    continue\n",
        "                cell_map.setdefault(rr, {})[cc] = cell.get(\"value\")\n",
        "            out[\"matrix\"] = [[cell_map.get(rr, {}).get(cc) for cc in c] for rr in r]\n",
        "        return drop_empty(out)\n",
        "\n",
        "    def reduce_bar(struct):\n",
        "        cats = struct.get(\"categories\") if isinstance(struct.get(\"categories\"), list) else None\n",
        "        series = struct.get(\"series\") if isinstance(struct.get(\"series\"), list) else None\n",
        "        out = {\"bar_chart_type\": struct.get(\"bar_chart_type\"), \"orientation\": struct.get(\"orientation\"), \"categories\": cats}\n",
        "        if cats and series:\n",
        "            out[\"series\"] = []\n",
        "            for s in series:\n",
        "                if not isinstance(s, dict):\n",
        "                    continue\n",
        "                data = s.get(\"data\") if isinstance(s.get(\"data\"), list) else []\n",
        "                mp = {}\n",
        "                for d in data:\n",
        "                    if not isinstance(d, dict):\n",
        "                        continue\n",
        "                    cat = d.get(\"category\")\n",
        "                    if cat is None:\n",
        "                        continue\n",
        "                    mp[cat] = d.get(\"value\")\n",
        "                out[\"series\"].append({\"label\": s.get(\"label\"), \"values\": [mp.get(c) for c in cats]})\n",
        "        return drop_empty(out)\n",
        "\n",
        "    def reduce_line(struct):\n",
        "        xl = struct.get(\"x_labels\") if isinstance(struct.get(\"x_labels\"), list) else None\n",
        "        series = struct.get(\"series\") if isinstance(struct.get(\"series\"), list) else None\n",
        "        out = {\"x_axis_type\": struct.get(\"x_axis_type\"), \"x_labels\": xl, \"y_unit\": struct.get(\"y_unit\")}\n",
        "        if xl and series:\n",
        "            out[\"series\"] = []\n",
        "            for s in series:\n",
        "                if not isinstance(s, dict):\n",
        "                    continue\n",
        "                pts = s.get(\"points\") if isinstance(s.get(\"points\"), list) else []\n",
        "                yvals = [None] * len(xl)\n",
        "                for i, p in enumerate(pts):\n",
        "                    if i >= len(xl) or not isinstance(p, dict):\n",
        "                        break\n",
        "                    yvals[i] = p.get(\"y_value\")\n",
        "                out[\"series\"].append({\"label\": s.get(\"label\"), \"y_values\": yvals})\n",
        "        return drop_empty(out)\n",
        "\n",
        "    def reduce_pie(struct):\n",
        "        slices = struct.get(\"slices\") if isinstance(struct.get(\"slices\"), list) else None\n",
        "        out = {\"context_label\": struct.get(\"context_label\"), \"is_donut_chart\": struct.get(\"is_donut_chart\")}\n",
        "        if slices:\n",
        "            out[\"slices\"] = [{\"label\": s.get(\"label\"), \"percentage\": s.get(\"percentage\")} for s in slices if isinstance(s, dict)]\n",
        "        return drop_empty(out)\n",
        "\n",
        "    def reduce_process(struct):\n",
        "        stages = struct.get(\"stages\") if isinstance(struct.get(\"stages\"), list) else None\n",
        "        out = {\"process_title\": struct.get(\"process_title\"), \"is_cycle\": struct.get(\"is_cycle\")}\n",
        "        if stages:\n",
        "            out[\"stages\"] = [{\"name\": s.get(\"name\"), \"order_index\": s.get(\"order_index\")} for s in stages if isinstance(s, dict)]\n",
        "        return drop_empty(out)\n",
        "\n",
        "    def reduce_map(struct):\n",
        "        out = {\"base_region_description\": struct.get(\"base_region_description\")}\n",
        "        sc = struct.get(\"scenarios\")\n",
        "        if isinstance(sc, list):\n",
        "            out[\"scenarios\"] = []\n",
        "            for s in sc:\n",
        "                if not isinstance(s, dict):\n",
        "                    continue\n",
        "                feats = s.get(\"features\") if isinstance(s.get(\"features\"), list) else []\n",
        "                f_out = []\n",
        "                for f in feats:\n",
        "                    if not isinstance(f, dict):\n",
        "                        continue\n",
        "                    f_out.append({\"label\": f.get(\"label\"), \"type\": f.get(\"type\"), \"category\": f.get(\"category\"), \"status\": f.get(\"status\")})\n",
        "                out[\"scenarios\"].append({\"label\": s.get(\"label\"), \"features\": f_out})\n",
        "        out[\"changes_between_scenarios\"] = struct.get(\"changes_between_scenarios\")\n",
        "        out[\"summary\"] = struct.get(\"summary\")\n",
        "        return drop_empty(out)\n",
        "\n",
        "    if not isinstance(meta, dict):\n",
        "        return meta\n",
        "    tcat = meta.get(\"task_visual_category\")\n",
        "    visuals = meta.get(\"visuals\") if isinstance(meta.get(\"visuals\"), list) else []\n",
        "    if len(visuals) >= 2 and tcat != \"multiple_graphs\":\n",
        "        tcat = \"multiple_graphs\"\n",
        "\n",
        "    out = {\n",
        "        \"schema_version\": meta.get(\"schema_version\"),\n",
        "        \"task_visual_category\": tcat,\n",
        "        \"topic_context\": meta.get(\"topic_context\"),\n",
        "    }\n",
        "    if isinstance(meta.get(\"global_semantics\"), dict):\n",
        "        gs = meta[\"global_semantics\"]\n",
        "        out[\"global_semantics\"] = {\n",
        "            \"overview\": gs.get(\"overview\"),\n",
        "            \"key_features\": gs.get(\"key_features\"),\n",
        "            \"extremes\": gs.get(\"extremes\"),\n",
        "            \"comparisons\": gs.get(\"comparisons\"),\n",
        "        }\n",
        "\n",
        "    v_out = []\n",
        "    for v in visuals:\n",
        "        if not isinstance(v, dict):\n",
        "            continue\n",
        "        vtype = v.get(\"visual_type\")\n",
        "        struct = v.get(\"structure\") if isinstance(v.get(\"structure\"), dict) else {}\n",
        "        if vtype == \"table\":\n",
        "            s_red = reduce_table(struct)\n",
        "        elif vtype == \"bar_chart\":\n",
        "            s_red = reduce_bar(struct)\n",
        "        elif vtype == \"line_graph\":\n",
        "            s_red = reduce_line(struct)\n",
        "        elif vtype == \"pie_chart\":\n",
        "            s_red = reduce_pie(struct)\n",
        "        elif vtype == \"process_diagram\":\n",
        "            s_red = reduce_process(struct)\n",
        "        elif vtype == \"map\":\n",
        "            s_red = reduce_map(struct)\n",
        "        else:\n",
        "            s_red = struct\n",
        "        v_out.append(drop_empty({\n",
        "            \"visual_id\": v.get(\"visual_id\"),\n",
        "            \"visual_type\": vtype,\n",
        "            \"role\": v.get(\"role\"),\n",
        "            \"panel_label\": v.get(\"panel_label\"),\n",
        "            \"title\": v.get(\"title\"),\n",
        "            \"structure\": s_red,\n",
        "        }))\n",
        "    if v_out:\n",
        "        out[\"visuals\"] = v_out\n",
        "\n",
        "    if isinstance(meta.get(\"relationships_between_visuals\"), list):\n",
        "        out[\"relationships_between_visuals\"] = meta.get(\"relationships_between_visuals\")\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "I0NiIzsXmiez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REDUCE_JSON = True\n",
        "\n",
        "# prompt builder\n",
        "def build_user(task_json_str: str, essay: str) -> str:\n",
        "    meta = json.loads(task_json_str)              # string -> dict\n",
        "    if REDUCE_JSON:\n",
        "        meta = reduce_task_json(meta)             # dict -> smaller dict\n",
        "    small_str = json.dumps(meta, ensure_ascii=False)  # dict -> string\n",
        "    return f\"TASK_PROMPT_JSON:\\n{small_str}\\n\\nCANDIDATE_ESSAY:\\n{essay}\\n\""
      ],
      "metadata": {
        "id": "jkgSosYMY6OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def extract_json(text: str):\n",
        "    text = text.strip()\n",
        "    i = text.find(\"{\")\n",
        "    if i == -1:\n",
        "        return None\n",
        "    try:\n",
        "        return json.JSONDecoder().raw_decode(text[i:])[0]\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "sSTVAKdcaUDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QWK needs integer labels. 0.0..9.0 step 0.5 => 0..18\n",
        "def round_half(x: float) -> float:\n",
        "    return round(x * 2) / 2"
      ],
      "metadata": {
        "id": "l7DlwU3acvIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an IELTS Academic Writing Task 1 examiner.\n",
        "\n",
        "You will receive TWO inputs:\n",
        "1) TASK_PROMPT_JSON: a structured JSON description of the Task 1 visual(s). Treat this as the ONLY ground truth.\n",
        "2) CANDIDATE_ESSAY: the candidate’s full written response.\n",
        "\n",
        "Your job: produce rubric-based band scores for IELTS Writing Task 1.\n",
        "\n",
        "SCORING SCALE (STRICT)\n",
        "- Score each criterion in 0.5 steps.\n",
        "- Criterion score range: 0.0 to 9.0 (inclusive).\n",
        "- All criterion scores must be multiples of 0.5.\n",
        "- IMPORTANT: overall_band_score must NEVER be 9.0. Cap overall_band_score at 8.5.\n",
        "\n",
        "CRITERIA (score all four)\n",
        "1) task_response_score (TR)\n",
        "   - Describe what is shown; no opinions/causes/solutions unless shown.\n",
        "   - MUST include a clear overview of main trends/major features (missing/unclear overview lowers TR).\n",
        "   - Select key features and comparisons; avoid listing everything.\n",
        "   - Accuracy is critical: penalize invented data, wrong figures/units/time periods, or trends that contradict TASK_PROMPT_JSON.\n",
        "2) coherence_cohesion_score (CC)\n",
        "   - Logical paragraphing (intro + overview + grouped details), clear progression, appropriate linking.\n",
        "3) lexical_resource_score (LR)\n",
        "   - Precise academic reporting vocabulary; accurate collocations for data (rise to/by, remain stable, peak at, etc.); avoid repetition.\n",
        "4) grammatical_range_accuracy_score (GRA)\n",
        "   - Range + accuracy; frequent errors and awkward structures reduce score.\n",
        "\n",
        "WORD COUNT RULE\n",
        "- If the essay is clearly under ~150 words, apply a noticeable penalty (especially TR, and often CC).\n",
        "\n",
        "LOW-SCORE VERIFICATION (IMPORTANT)\n",
        "If your initial scoring suggests ANY criterion < 4.5, you MUST do a second, rigorous check BEFORE finalizing:\n",
        "A) Re-check TR basics: is there at least an attempt at paraphrase + overview + some data/features (even if weak)?\n",
        "B) Re-check whether errors are truly severe enough to justify <4.5 versus a weak-but-present response (≈4.5–5.0).\n",
        "C) Re-check that you are not over-penalizing for grammar/vocabulary when the task meaning is still recoverable.\n",
        "D) Only keep a score <4.5 if the response is clearly extremely limited (e.g., no real overview, very little/incorrect description, heavy invention, or meaning mostly unclear).\n",
        "\n",
        "OVERALL BAND (STRICT)\n",
        "- overall_band_score = average of the four criterion scores.\n",
        "- Round to the nearest 0.5.\n",
        "- If exactly halfway between two 0.5 steps (x.25 or x.75), round UP.\n",
        "- After rounding, if overall_band_score == 9.0, set overall_band_score = 8.5.\n",
        "\n",
        "OUTPUT FORMAT (STRICT)\n",
        "Return ONLY one valid JSON object with exactly these keys and numeric values (no extra keys, no explanation, no markdown, no surrounding text).\n",
        "\n",
        "{\n",
        "  \"overall_band_score\": number,\n",
        "  \"task_response_score\": number,\n",
        "  \"coherence_cohesion_score\": number,\n",
        "  \"lexical_resource_score\": number,\n",
        "  \"grammatical_range_accuracy_score\": number\n",
        "}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "ZPkJ9xVJ2Z56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "pred_scores = []\n",
        "bad_outputs = []   # optional\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    task_json_string = row[\"image\"] # Use the original string\n",
        "\n",
        "    essay = row[\"content\"]\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": build_user(task_json_string, essay)}, # Pass the string here\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(**inputs, max_new_tokens=250, do_sample=False)\n",
        "\n",
        "    gen_ids = out[0][inputs[\"input_ids\"].shape[-1]:]   # only new tokens\n",
        "    decoded = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "    pred = extract_json(decoded)\n",
        "\n",
        "\n",
        "    pred = extract_json(decoded)\n",
        "    if isinstance(pred, dict):\n",
        "        pred = {k: pred.get(k) for k in KEYS}\n",
        "        pred_scores.append(json.dumps(pred, ensure_ascii=False))\n",
        "    else:\n",
        "        pred_scores.append(\"\")\n",
        "        bad_outputs.append((idx, decoded[:500]))  # optional preview\n",
        "\n",
        "pred_test_df = test_df[[\"image\", \"content\"]].copy()\n",
        "pred_test_df[\"pred_scores\"] = pred_scores\n",
        "\n",
        "OUT_CSV = \"/content/drive/MyDrive/datasets/splits/pred_test.csv\"\n",
        "pred_test_df.to_csv(OUT_CSV, index=False)\n",
        "print(\"Saved:\", OUT_CSV)\n",
        "\n",
        "print(\"Failed JSON rows:\", len(bad_outputs))\n",
        "print(\"Example failed output:\", bad_outputs[0] if bad_outputs else \"None\")"
      ],
      "metadata": {
        "id": "jCOzB9SO3iv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "tokenizer.truncation_side = \"left\"\n",
        "\n",
        "row = test_df.iloc[0]\n",
        "task_meta = json.loads(row[\"image\"])\n",
        "task_small = reduce_task_json(task_meta)\n",
        "task_json_str = json.dumps(task_small, ensure_ascii=False)\n",
        "essay = row[\"content\"]"
      ],
      "metadata": {
        "id": "dtC81axMY0sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": build_user(task_json_str, essay)},\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)"
      ],
      "metadata": {
        "id": "0Tsc2cUqIdtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    out = model.generate(**inputs, max_new_tokens=250, do_sample=False)"
      ],
      "metadata": {
        "id": "mgbss4qbIgSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_ids = out[0][inputs[\"input_ids\"].shape[-1]:]   # only new tokens\n",
        "decoded = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "pred = extract_json(decoded)"
      ],
      "metadata": {
        "id": "0yP9bc5PIoPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded"
      ],
      "metadata": {
        "id": "umHoWPxEIyWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "id": "LVZmVGbSI1PN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}